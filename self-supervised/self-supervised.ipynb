{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.20.1)\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/cu118/torchaudio-2.5.1%2Bcu118-cp312-cp312-win_amd64.whl (4.0 MB)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (70.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (1.26.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/cu118/torch-2.5.1%2Bcu118-cp312-cp312-win_amd64.whl (2700.1 MB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Installing collected packages: torch, torchaudio\n",
      "Successfully installed torch-2.5.1+cu118 torchaudio-2.5.1+cu118\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --user torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3373289064.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "##installs pytorch on a cuda-capable windows machine using pip\n",
    "\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "!pip install torchsummary\n",
    "\n",
    "!pip install numpy\n",
    "\n",
    "!pip install matplotlib\n",
    "\n",
    "!pip install opencv-python\n",
    "\n",
    "!pip install scikit-learn\n",
    "\n",
    "!pip install pandas\n",
    "\n",
    "!pip install pickle5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: progettoVIPM\n"
     ]
    }
   ],
   "source": [
    "if(os.path.split(os.getcwd())[1] == \"Transfer_Learning\" or os.path.split(os.getcwd())[1] == \"semi-supervised\" or os.path.split(os.getcwd())[1] == \"self-supervised\"):\n",
    "    os.chdir(\"..\")\n",
    "print(\"Current Working Directory: {}\".format(os.path.split(os.getcwd())[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from enum import Enum\n",
    "\n",
    "class datasets(Enum):\n",
    "    TRAINING_LABELED = [\"train_small.csv\", \"train_set\"]\n",
    "    TRAINING_UNLABELED = [\"train_unlabeled.csv\", \"train_set\"]\n",
    "    TEST = [\"val_info.csv\", \"val_set\"]\n",
    "    TEST_DEGRADED = [\"val_info.csv\", \"val_set_degraded\"]\n",
    "\n",
    "class networks(Enum):\n",
    "    ALEXNET = [227, models.alexnet(pretrained=True), \"AlexNet\"] #227?\n",
    "    RESNET50 = [224, models.resnet50(pretrained=True), \"ResNet50\"]\n",
    "    GOOGLENET = [224, models.googlenet(pretrained=True), \"GoogLeNet\"]\n",
    "    MOBILENET = [224, models.mobilenet_v3_small(pretrained=True), \"mobilenet_v3_small\"]\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataset, network_input_size):\n",
    "        super().__init__()\n",
    "        self.images_names = []\n",
    "        self.labels = []\n",
    "        dataset = dataset.value\n",
    "        annotations_file = dataset[0]\n",
    "        img_dir = dataset[1]\n",
    "        with open(annotations_file, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for row in reader:\n",
    "                self.images_names.append(\"./{}/{}\".format(img_dir, row[0]))\n",
    "                self.labels.append(row[1])\n",
    "        self.images_names = np.array(self.images_names)\n",
    "        self.labels = np.array(self.labels)\n",
    "        # in base al valore passato si sceglie la rete che utilizzerà il dataset, serve per modificare le dimensioni delle immagini\n",
    "        self.im_size = network_input_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #moveaxis serve per avere come dimensione dell'immagine (3, righe, colonne) invece di (righe, colonne, 3)\n",
    "        image = np.moveaxis(cv2.resize((cv2.imread(self.images_names[index], cv2.IMREAD_COLOR).astype(np.double)/255), \n",
    "                                       (self.im_size,self.im_size), \n",
    "                                        interpolation=cv2.INTER_CUBIC).astype(np.float32),\n",
    "                            -1, 0)\n",
    "        # eventualmente si può aggiungere l'alternativa di fare random cropping dell'immagine\n",
    "        label = self.labels[index]                \n",
    "        if(cuda):\n",
    "            return torch.from_numpy(image).cuda(), label\n",
    "        else:\n",
    "            return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2000/113455 [00:35<33:20, 55.71it/s] \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset_holder = ImageDataset(dataset=datasets.TRAINING_UNLABELED, network_input_size=256)\n",
    "loader = DataLoader(dataset=dataset_holder, shuffle=False, batch_size=1)\n",
    "\n",
    "imagesToRotate=2000\n",
    "i=0\n",
    "rotated_images=[]\n",
    "rotated_images_y=[]\n",
    "orientations = [0, 90, 180, 270]\n",
    "for image, y in tqdm(loader):\n",
    "    i=i+1\n",
    "    if i>imagesToRotate: break\n",
    "    for angle in orientations:\n",
    "        rotated_image = transforms.functional.rotate(image, angle)\n",
    "        rotated_images.append(rotated_image)\n",
    "        rotated_images_y.append(int(angle/90))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rotated_images_y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     one_hot_labels[np\u001b[38;5;241m.\u001b[39marange(labels\u001b[38;5;241m.\u001b[39msize), labels] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m one_hot_labels\n\u001b[1;32m----> 7\u001b[0m rotated_images_y \u001b[38;5;241m=\u001b[39m one_hot_encode(np\u001b[38;5;241m.\u001b[39marray(\u001b[43mrotated_images_y\u001b[49m), \u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rotated_images_y' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert labels to one-hot encoding\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    one_hot_labels = np.zeros((labels.size, num_classes))\n",
    "    one_hot_labels[np.arange(labels.size), labels] = 1\n",
    "    return one_hot_labels\n",
    "\n",
    "rotated_images_y = one_hot_encode(np.array(rotated_images_y), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Batch 100] loss: 0.784\n",
      "[Epoch 1, Batch 200] loss: 0.476\n",
      "[Epoch 2, Batch 100] loss: 0.330\n",
      "[Epoch 2, Batch 200] loss: 0.328\n",
      "[Epoch 3, Batch 100] loss: 0.229\n",
      "[Epoch 3, Batch 200] loss: 0.254\n",
      "[Epoch 4, Batch 100] loss: 0.158\n",
      "[Epoch 4, Batch 200] loss: 0.193\n",
      "[Epoch 5, Batch 100] loss: 0.121\n",
      "[Epoch 5, Batch 200] loss: 0.150\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load the pretrained mobilenet_v3_small model\n",
    "model = models.mobilenet_v3_small(pretrained=True)\n",
    "\n",
    "# Modify the classifier to fit 4 classes\n",
    "model.classifier[3] = nn.Linear(model.classifier[3].in_features, 4)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Prepare the dataset and dataloader\n",
    "# Assuming rotated_images is a list of tensors with shape [3, 256, 256] (RGB images) WRONG ASSUMPTION WRONG ASSUMPTION WRONG ASSUMPTION WRONG ASSUMPTION\n",
    "rotated_images_tensor = torch.stack(rotated_images)  # Stack tensors to shape [batch_size, 3, 256, 256]\n",
    "rotated_images_y_tensor = torch.tensor(rotated_images_y, dtype=torch.long)\n",
    "\n",
    "dataset = TensorDataset(rotated_images_tensor, rotated_images_y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)  # Adjust batch size as needed\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(torch.squeeze(inputs))\n",
    "        loss = criterion(outputs.to(torch.float ), labels.to(torch.float ))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # Print every 100 mini-batches\n",
    "            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from enum import Enum\n",
    "\n",
    "class datasets(Enum):\n",
    "    TRAINING_LABELED = [\"train_small.csv\", \"train_set\"]\n",
    "    TRAINING_UNLABELED = [\"train_unlabeled.csv\", \"train_set\"]\n",
    "    TEST = [\"val_info.csv\", \"val_set\"]\n",
    "    TEST_DEGRADED = [\"val_info.csv\", \"val_set_degraded\"]\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataset, network_input_size):\n",
    "        super().__init__()\n",
    "        self.images_names = []\n",
    "        self.labels = []\n",
    "        dataset = dataset.value\n",
    "        annotations_file = dataset[0]\n",
    "        img_dir = dataset[1]\n",
    "        with open(annotations_file, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for row in reader:\n",
    "                self.images_names.append(\"./{}/{}\".format(img_dir, row[0]))\n",
    "                self.labels.append(row[1])\n",
    "        self.images_names = np.array(self.images_names)\n",
    "        self.labels = np.array(self.labels)\n",
    "        # in base al valore passato si sceglie la rete che utilizzerà il dataset, serve per modificare le dimensioni delle immagini\n",
    "        self.im_size = network_input_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #moveaxis serve per avere come dimensione dell'immagine (3, righe, colonne) invece di (righe, colonne, 3)\n",
    "        image = np.moveaxis(cv2.resize((cv2.imread(self.images_names[index], cv2.IMREAD_COLOR).astype(np.double)/255), \n",
    "                                       (self.im_size,self.im_size), \n",
    "                                        interpolation=cv2.INTER_CUBIC).astype(np.float32),\n",
    "                            -1, 0)\n",
    "        # eventualmente si può aggiungere l'alternativa di fare random cropping dell'immagine\n",
    "        label = self.labels[index]                \n",
    "        if(cuda):\n",
    "            return torch.from_numpy(image).cuda(), label\n",
    "        else:\n",
    "            return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "model2 = deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Open a file and use dump() \n",
    "with open('modelTrainedOnRotated.pkl', 'wb') as file:       \n",
    "    # A new file will be created \n",
    "    pickle.dump(model, file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('modelTrainedOnRotated.pkl', 'rb') as file: \n",
    "      \n",
    "    # Call load method to deserialze \n",
    "    model2 = pickle.load(file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 2000/5020 [00:10<00:16, 184.56it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_holder = ImageDataset(dataset=datasets.TRAINING_LABELED, network_input_size=256)\n",
    "loader = DataLoader(dataset=dataset_holder, shuffle=False, batch_size=1)\n",
    "images =[]\n",
    "images_y=[]\n",
    "numOfImagesToFinetune=2000\n",
    "i=0\n",
    "for image, y in tqdm(loader):\n",
    "    i=i+1\n",
    "    if i>numOfImagesToFinetune: break    \n",
    "    images.append(image)\n",
    "    images_y.append(int(y[0]))\n",
    "\n",
    "\n",
    "images_y = one_hot_encode(np.array(images_y), 251)\n",
    "\n",
    "images_tensor = torch.stack(images)  # Stack tensors to shape [batch_size, 3, 256, 256]\n",
    "images_y_tensor = torch.tensor(images_y, dtype=torch.long)\n",
    "\n",
    "dataset = TensorDataset(images_tensor, images_y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)  # Adjust batch size as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the model2 to have 251 output classes\n",
    "model2.classifier = nn.Linear(576, 251)\n",
    "\n",
    "# Define the new loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model2.parameters(), lr=0.001)\n",
    "\n",
    "# Fine-tuning loop\n",
    "num_epochs = 5  # Adjust the number of epochs as needed\n",
    "model2.train()\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model2(torch.squeeze(inputs))\n",
    "        loss = criterion(outputs.to(torch.float ), labels.to(torch.float ))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # Print every 100 mini-batches\n",
    "            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 3000/5020 [00:40<00:27, 73.45it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataset_holder = ImageDataset(dataset=datasets.TEST, network_input_size=256)\n",
    "test_loader = DataLoader(dataset=test_dataset_holder, shuffle=False, batch_size=1)\n",
    "test_images =[]\n",
    "test_images_y=[]\n",
    "numOfImagesToTestWith=3000\n",
    "i=0\n",
    "for image, y in tqdm(loader):\n",
    "    i=i+1\n",
    "    if i>numOfImagesToTestWith: break    \n",
    "    test_images.append(image)\n",
    "    test_images_y.append(int(y[0]))\n",
    "\n",
    "\n",
    "test_images_y = one_hot_encode(np.array(test_images_y), 251)\n",
    "\n",
    "test_images_tensor = torch.squeeze(torch.stack(test_images))  # Stack tensors to shape [batch_size, 3, 256, 256]\n",
    "test_images_y_tensor = torch.tensor(test_images_y, dtype=torch.long)\n",
    "\n",
    "test_dataset = TensorDataset(test_images_tensor, test_images_y_tensor)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)  # Adjust batch size as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "guessedTestLabels = model2(test_images_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "decodedProba=[]\n",
    "for e in guessedTestLabels:\n",
    "    decodedProba.append(np.argmax(e.detach().numpy()))\n",
    "\n",
    "decodedTestImagesY=[]\n",
    "for e in guessedTestLabels:\n",
    "    decodedTestImagesY.append(np.argmax(e.detach().numpy()))\n",
    "\n",
    "\n",
    "score = accuracy_score(decodedTestImagesY, decodedProba)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: progettoVIPM\n"
     ]
    }
   ],
   "source": [
    "if(os.path.split(os.getcwd())[1] == \"misc\"):\n",
    "    os.chdir(\"..\")\n",
    "print(\"Current Working Directory: {}\".format(os.path.split(os.getcwd())[1]))\n",
    "\n",
    "cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "_TYPES = Literal[\"highest\", \"lowest\"]\n",
    "\n",
    "class Confusion_Matrix_stats():\n",
    "    def __init__(self, filename, classes):\n",
    "        \n",
    "        # Load the data from file\n",
    "\n",
    "        self.filename = filename\n",
    "\n",
    "        if \".npy\" in filename:\n",
    "            print(\"File is readable by numpy\")\n",
    "            self.cm = np.load(filename).astype(\"float32\")\n",
    "        if \".csv\" in filename:\n",
    "            print(\"File is readable by csv\")\n",
    "            self.cm = np.loadtxt(filename, delimiter=\",\", dtype=np.int64)\n",
    "        \n",
    "        \n",
    "        self.classes = classes\n",
    "\n",
    "        # Compute basic metrics\n",
    "\n",
    "        self.accuracy = self.cm.diagonal()/self.cm.sum(axis=1)\n",
    "\n",
    "        self.total_accuracy = self.cm.diagonal().sum() / self.cm.sum()\n",
    "        \n",
    "        # the following matrix is used to compute both the false negative and false positives by using dot prodct\n",
    "        # the values will be retrieved from the diagonal of the obtained matrix\n",
    "        prod_matrix = np.ones(self.cm.shape, dtype=np.int8)\n",
    "        np.fill_diagonal(prod_matrix, 0)\n",
    "\n",
    "        self.false_negative = np.dot(self.cm, prod_matrix).diagonal()\n",
    "\n",
    "        self.false_positive = np.dot(np.transpose(self.cm), prod_matrix).diagonal()\n",
    "\n",
    "        self.true_positive = self.cm.diagonal()\n",
    "\n",
    "        self.real_positive = np.sum(self.cm, axis=1)\n",
    "\n",
    "        self.real_negative = np.sum(self.cm, axis=0)\n",
    "\n",
    "        # the true negative values are computed by excluding the rest of the values from the total sum of the matrix\n",
    "\n",
    "        self.true_negative = -self.real_negative + self.cm.sum() - self.real_positive + self.true_positive\n",
    "\n",
    "        self.predicted_positive = self.true_positive + self.false_positive\n",
    "\n",
    "        self.predicted_negative = self.false_negative + self.true_negative\n",
    "\n",
    "        # Compute advanced metrics\n",
    "\n",
    "        self.precision = self.true_positive / self.predicted_positive\n",
    "\n",
    "        self.recall = self.true_positive/self.real_positive\n",
    "\n",
    "        self.f1 = 2*(self.precision * self.recall)/(self.precision + self.recall)\n",
    "\n",
    "    def show_top_k_confused_classes(self, k):\n",
    "        # This function returns the top k confused classes as a list of double values\n",
    "        # the returned list is bicriterional in the sense that it returns the top k confused classes in both ways.\n",
    "        # As an example: if the couple (1,2) is returned it means that the total amount of cases in which 1 was confused with 2\n",
    "        # and vice versa was great\n",
    "\n",
    "        \n",
    "        # The following matrix \"summed_mat\" is initialized, it will eventually contain\n",
    "        # the sums of the rows and columns of the confusion mat of each class\n",
    "        # This is basically accomplished by summing the upper triangular part of the matrix with the lower triangular part\n",
    "        # the height of the matrix is set as self.cm.shape[0]-1 because we do self.cm.shape[0]-1 iterations\n",
    "        # since at every iteration the remaining parts of the matrix to confront shrinks when the last row\n",
    "        # is reached there will be no confront to make\n",
    "        summed_mat = np.zeros((self.cm.shape[0]-1, self.cm.shape[1]), dtype=np.int64)\n",
    "        for i in range(self.cm.shape[0]-1):\n",
    "            # sum the errors in the row and column relative to the current class i\n",
    "            i_row = self.cm[i, i+1:]\n",
    "            i_column = self.cm[i+1:, i]\n",
    "            i_row = i_row + i_column\n",
    "            # there is a padding of -1s in order to obtain a vector of correct lenght.\n",
    "            # -1 are chosen because when doing argmax operation they will never be chosen, since in self.cm the minimum value is 0\n",
    "            i_row = np.pad(i_row, (i+1,0), 'constant', constant_values=(-1))\n",
    "            summed_mat[i] = i_row\n",
    "        top_k= []\n",
    "        print(\"These are the top k confused couples of classes\")\n",
    "        for i in range(k):\n",
    "            # unravel_index is needed because np.argmax returns the position of the maximum value in the flattened array\n",
    "            pos = np.unravel_index(np.argmax(summed_mat, axis=None), summed_mat.shape)\n",
    "            p = (self.classes[pos[0]], self.classes[pos[1]])\n",
    "            print(\"#{}: {}; has been confused {} times\".format(i+1, p, np.max(summed_mat)))\n",
    "            top_k.append(pos)\n",
    "            # after printing ad appending the couple we set the value at its position as -1 in order to not choose it again\n",
    "            summed_mat[pos]=-1\n",
    "        return top_k\n",
    "\n",
    "\n",
    "\n",
    "    def show_metrics(self, index):\n",
    "        \n",
    "        print(\"---------------------------------------------------------\")\n",
    "        print(\"Showing metrics of class: {}; filename:{}\".format(self.classes[index], self.filename))\n",
    "        print(\"Total accuracy of given cm: {} %\".format(self.total_accuracy * 100))\n",
    "        print(\"-------------------basic metrics------------------------\")\n",
    "\n",
    "        print(\"Accuracy: {} %\".format(self.accuracy[index] * 100))\n",
    "\n",
    "        print(\"N° of real positives: {}\".format(self.real_positive[index]))\n",
    "        print(\"N° of true positives: {}\".format(self.true_positive[index]))\n",
    "        print(\"N° of false negatives: {}\".format(self.false_negative[index]))\n",
    "\n",
    "        print(\"N° of real negatives: {}\".format(self.real_negative[index]))\n",
    "        print(\"N° of true negatives: {}\".format(self.true_negative[index]))\n",
    "        print(\"N° of false positives: {}\".format(self.false_positive[index]))\n",
    "        \n",
    "        print(\"N° of predicted positives: {}\".format(self.predicted_positive[index]))\n",
    "        print(\"N° of predicted negatives: {}\".format(self.predicted_negative[index]))\n",
    "\n",
    "        print(\"-------------------advanced metrics----------------------\")\n",
    "\n",
    "        print(\"Precision: {}\".format(self.precision[index]))\n",
    "        print(\"Recall: {}\".format(self.recall[index]))\n",
    "        print(\"F1-score: {}\".format(self.f1[index]))\n",
    "\n",
    "        print(\"---------------------------------------------------------\")\n",
    "    \n",
    "    def show_metrics_k_acc(self, k, criterion: _TYPES =\"highest\"):\n",
    "        index_best = np.argsort(self.accuracy)[:k]\n",
    "        if criterion == \"lowest\":\n",
    "            index_best = np.argsort(self.accuracy)[-k:]\n",
    "        print(\"Showing metrics of {} {} accuracy classes\".format(criterion, k))\n",
    "        for index in index_best:\n",
    "            self.show_metrics(index)\n",
    "    \n",
    "    def show_metrics_k_tp(self, k, criterion: _TYPES =\"highest\"):\n",
    "        index_best = np.argsort(self.true_positive)[:k]\n",
    "        if criterion == \"lowest\":\n",
    "            index_best = np.argsort(self.true_positive)[-k:]\n",
    "        print(\"Showing metrics of {} {} true positives classes\".format(criterion, k))\n",
    "        for index in index_best:\n",
    "            self.show_metrics(index)\n",
    "    \n",
    "    def show_metrics_k_tn(self, k, criterion: _TYPES =\"highest\"):\n",
    "        index_best = np.argsort(self.true_negative)[:k]\n",
    "        if criterion == \"lowest\":\n",
    "            index_best = np.argsort(self.true_negative)[-k:]\n",
    "        print(\"Showing metrics of {} {} true negatives classes\".format(criterion, k))\n",
    "        for index in index_best:\n",
    "            self.show_metrics(index)\n",
    "    \n",
    "    def show_metrics_k_precision(self, k, criterion: _TYPES =\"highest\"):\n",
    "        index_best = np.argsort(self.precision)[:k]\n",
    "        if criterion == \"lowest\":\n",
    "            index_best = np.argsort(self.precision)[-k:]\n",
    "        print(\"Showing metrics of {} {} precision classes\".format(criterion, k))\n",
    "        for index in index_best:\n",
    "            self.show_metrics(index)\n",
    "    \n",
    "    def show_metrics_k_recall(self, k, criterion: _TYPES =\"highest\"):\n",
    "        index_best = np.argsort(self.recall)[:k]\n",
    "        if criterion == \"lowest\":\n",
    "            index_best = np.argsort(self.recall)[-k:]\n",
    "        print(\"Showing metrics of {} {} recall classes\".format(criterion, k))\n",
    "        for index in index_best:\n",
    "            self.show_metrics(index)\n",
    "    \n",
    "    def show_metrics_k_f1(self, k, criterion: _TYPES =\"highest\"):\n",
    "        index_best = np.argsort(self.f1)[:k]\n",
    "        if criterion == \"lowest\":\n",
    "            index_best = np.argsort(self.f1)[-k:]\n",
    "        print(\"Showing metrics of {} {} f1-score classes\".format(criterion, k))\n",
    "        for index in index_best:\n",
    "            self.show_metrics(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File is readable by numpy\n",
      "---------------------------------------------------------\n",
      "Showing metrics of class: beignet; filename:./Transfer_Learning/model_metrics/ConfM_finetunedResNet50_minusnessuno_60e_64bsize_80_20_split_test_dataset.npy\n",
      "Total accuracy of given cm: 17.23361611366272 %\n",
      "-------------------basic metrics------------------------\n",
      "Accuracy: 9.836065769195557 %\n",
      "N° of real positives: 61.0\n",
      "N° of true positives: 6.0\n",
      "N° of false negatives: 55.0\n",
      "N° of real negatives: 14.0\n",
      "N° of true negatives: 11925.0\n",
      "N° of false positives: 8.0\n",
      "N° of predicted positives: 14.0\n",
      "N° of predicted negatives: 11980.0\n",
      "-------------------advanced metrics----------------------\n",
      "Precision: 0.4285714328289032\n",
      "Recall: 0.09836065769195557\n",
      "F1-score: 0.1599999964237213\n",
      "---------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adriano\\AppData\\Local\\Temp\\ipykernel_32592\\318091129.py:53: RuntimeWarning: invalid value encountered in divide\n",
      "  self.precision = self.true_positive / self.predicted_positive\n",
      "C:\\Users\\Adriano\\AppData\\Local\\Temp\\ipykernel_32592\\318091129.py:57: RuntimeWarning: invalid value encountered in divide\n",
      "  self.f1 = 2*(self.precision * self.recall)/(self.precision + self.recall)\n"
     ]
    }
   ],
   "source": [
    "filename = \"./Transfer_Learning/model_metrics/{}.npy\".format(\"ConfM_finetunedResNet50_minusnessuno_60e_64bsize_80_20_split_test_dataset\")\n",
    "classes = np.loadtxt(\"disambiguation.csv\", delimiter=\",\", dtype=\"str\")[:,1]\n",
    "# classes = [0,1,2,3]\n",
    "confusion_matrix = Confusion_Matrix_stats(filename, classes)\n",
    "confusion_matrix.show_metrics(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the top k confused couples of classes\n",
      "#1: ('lasagna', 'cannelloni'); has been confused 24 times\n",
      "#2: ('bruschetta', 'stuffed_tomato'); has been confused 23 times\n",
      "#3: ('cannelloni', 'manicotti'); has been confused 22 times\n",
      "#4: ('chicken_wing', 'buffalo_wing'); has been confused 21 times\n",
      "#5: ('miso_soup', 'hot_and_sour_soup'); has been confused 20 times\n",
      "#6: ('strudel', 'grilled_cheese_sandwich'); has been confused 19 times\n",
      "#7: ('hot_and_sour_soup', 'pho'); has been confused 18 times\n",
      "#8: ('souffle', 'creme_brulee'); has been confused 17 times\n",
      "#9: ('cruller', 'onion_rings'); has been confused 16 times\n",
      "#10: ('chow_mein', 'spaghetti_bolognese'); has been confused 16 times\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(98, 182),\n",
       " (150, 250),\n",
       " (182, 192),\n",
       " (93, 249),\n",
       " (135, 186),\n",
       " (160, 201),\n",
       " (186, 204),\n",
       " (46, 62),\n",
       " (2, 88),\n",
       " (16, 70)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix.show_top_k_confused_classes(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.arange(251)\n",
    "\n",
    "rows = [\"{}\".format(i) for i in classes] \n",
    "text = \"\\n\".join(rows) \n",
    "  \n",
    "with open('disambiguation.csv', 'w') as f: \n",
    "    f.write(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

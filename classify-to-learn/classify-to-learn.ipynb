{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: progettoVIPM\n"
     ]
    }
   ],
   "source": [
    "if(os.path.split(os.getcwd())[1] == \"classify-to-learn\"):\n",
    "    os.chdir(\"..\")\n",
    "print(\"Current Working Directory: {}\".format(os.path.split(os.getcwd())[1]))\n",
    "\n",
    "cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loadersAndEnums import datasets\n",
    "from utils.loadersAndEnums import networks\n",
    "from utils.loadersAndEnums import ImageDataset\n",
    "from utils.v2 import ExtendedEncoder, Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loadersAndEnums import datasets, ImageDataset\n",
    "from torch.utils.data import DataLoader, ConcatDataset, random_split\n",
    "\n",
    "trainL = ImageDataset(dataset=datasets.TRAINING_LABELED,network_input_size=256, cuda=cuda)\n",
    "trainU = ImageDataset(dataset=datasets.TRAINING_UNLABELED,network_input_size=256, cuda=cuda)\n",
    "#trainC = ConcatDataset([trainL,trainU])\n",
    "trainC = trainL #doing it properly... it is necessary to do proper validation that the lavidation data are unseen during training of the model...\n",
    "train_size = int(0.8*len(trainC))\n",
    "val_size = len(trainC)-train_size\n",
    "train,val = random_split(trainC,[train_size,val_size])\n",
    "test = ImageDataset(dataset=datasets.TEST,network_input_size=256,cuda=cuda)\n",
    "\n",
    "\n",
    "trainL = DataLoader(trainL,128,True)\n",
    "train = DataLoader(train,128,True)\n",
    "val = DataLoader(val,128,True)\n",
    "test = DataLoader(test,128,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['macaron', 'bignet', 'cruller', 'vongole_clams', 'triangoli_fritti', 'tiramisu', 'mexican_tostadas', 'lamb_moussaka', 'ravioli_cinesi', 'sushi_sashimi', 'knishes', 'crocchette', 'couscous_verdure', 'porridge', 'lattuga_kimchi', 'alga_wakame', 'spaghetti_squash', 'pasta_rigatoni', 'tartare_di_carne', 'cannolo_siciliano', 'fegato_oca', 'cupcakes', 'carne_ossobuco', 'tagliatelle_con_frutti_di_mare', 'canadian_poutine_w_mushroom_gravy', 'ramen', 'panino_pulled_pork', 'ramen_con_uovo_in_mezzo', 'chicken_kiev', 'torta_mele_stile_crostata', 'risotto', 'torta_stile_torrone_canditi', 'maccheroni_con_carne', 'gluten_free_haggis', 'uova_strapazzate_colazione', 'frittata', 'shrimp_scampi', 'sushi_maki', 'orzo_bolognese', 'amish_apple_fritters', 'nachos', 'beef_stroganoff', 'beef_wellington', 'involtino_primavera', 'dolce_rotondo_con_buco', 'gamberoni', 'souffle', 'maiale_cinese', 'muffin', 'uovo_sodo_con_mousse', 'escargots', 'tramezzino_pollo_club_sandwich', 'torta_di_carote', 'falafell', 'farfalle_pasta', 'smoked_duck_terrine', 'uovo_in_camicia', 'gnocchi', 'bubble_n_squek', 'egg_rolls_invol_primav', 'caprese_non_la_pizza', 'sauerkraut', 'creme_brule', 'pavlova', 'fonduta', 'clam', 'jambalaya', 'tempura', 'torta_al_cioccolato', 'chicken_pot_pie', 'pasta_ragu', 'hotpot_giapponese', 'pure_di_mele', 'baklava', 'salisbury_steak', 'spaghetti', 'edamame', 'coq_au_vin', 'tamale', 'mac_n_cheese', 'riso_con_uovo', 'garlic_bread', 'inslata_con_barbabietole', 'tartare_di_carne', 'vermicelli', 'patÃ©_di_pollo', 'pancake', 'chicken_teraggini_stew', 'anelli_di_cipolla', 'red_velvet', 'berry_compote', 'aragosta', 'butter_chicken', 'alette_di_pollo', 'ceaser_salad', 'Succotash', 'hummus', 'fish_n_chips', 'lasagna', 'lutefisk', 'sloppy_joes', 'omino_di_pandizenzero', 'crocchetta_di_pollo_fritto', 'carne_brasato_cube', 'maiale_glassato_cinese', 'guacamole', 'cheddar_sandwich', 'pancake_coi_buchi', 'tacos', 'torta_con_fragole', 'clam_chouder', 'patate_al_forno_con_sotto_il_ragu', 'uovo_sul_pane', 'zuppa_con_formaggio_in_una_ciotolina', 'fettine_di_tartare', 'torta', '???_poi', 'chele_di_granchio', 'uovo_colazione_con_bacon', 'torta_con_glassa', 'custard', 'syllabub', 'porkchop', 'riso_alla_cantonese', 'uovo_sodo', 'chicken_galatine', 'bbq_brisket', 'reuben_sandwich', 'cotoletta', 'ambrosia_salad', 'ravioli_cinesi_abbrustoliti', 'beef_jerky', 'ravioli', 'anelli_di_totano', 'spaghetti_carbonara', 'miso_soup', 'frozen_yoghurt', 'ahi_wonton', 'pannacotta', 'french_toast', 'chicken_enchilada', 'poke', 'fettucine_alfredo', 'chili', 'budino_caramello', 'spiedino_di_pollo', 'sponge_cake', 'casserole', 'riso_frutti_di_mare_cozze', 'blancmange', 'bruschetta', 'tortellini', 'salmone_alla_griglia', 'patatine_fritte', 'shrimp_n_grits', 'churros', 'donout', 'polpettone', 'polpette', 'scrapple', 'strudel', 'torta_bounty', 'torta_swirls', 'filet_mignon', 'hamburger', 'uovo_in_padella', 'tartare', 'pasta_penne', 'egg_benedict', 'torta_di_pane', 'takoyaki', 'smoked_pork_tenderloin', 'mousse_di_cioccolato', 'meringa', 'hotdog', 'cibo_fancy', 'piatti_con_uovo', 'pollo_riso', 'insalata_greca', 'rancheros', 'tagliatelle', 'pomodoro_al_forno_ripieno', 'pasta_al_forno', 'pizza', 'rustici', 'tortilla', 'thai_soup', 'prime_ribs', 'cheesecake', 'clams', 'baked_ziti', 'cozze', 'manicotti', 'gelato', 'waffle', 'ostriche', 'omelette', 'pasta_vongole', 'burrito', 'rotolo', 'zuppa_aragosta', 'grilled_cheese', 'pita_gyro', 'plumcake', 'zuppa_spaghetti_riso', 'panino_aragosta', 'bbq_ribs', 'pate_di_olive', 'chinese_pepper_steak', 'pane_con_formaggio_sopra', 'pilaf', 'foglie_ripiene', 'scaloppine', 'cordon_bleu', 'uovo_denver_omlette', 'alette_di_pollo', 'gamberoni_ripieni', 'carne_con_pepe_cremina', 'frutto_di_mare_non_identificato', 'chifon_cake', 'bignet', 'toad_in_the_hole', 'chicken_marengo', 'victoria_sponge', 'chicken_tamale', 'creampie', 'bastoncini_findus', 'crumb_cake', 'chicken_provencal', 'pizzette', 'bistecca', 'breasato', 'panino', 'scotch_eggs', 'ravioloni', 'peach_melba', 'pear_cake', 'pumpkin_applesauce_cake', 'dont_know', 'torta_a_piu_strati', 'bbq_ribs', 'beef_bourguignon', 'rissoles', 'pasta_al_forno', 'flacky_turnovers', 'canederli', 'cordon_bleu', 'dolcetti_pasta_sfoglia', 'piatto_cinese_verdure_gamberi_agrodolce', 'buffalo_wings', 'pomodoro_al_forno_ripieno']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "translate_label = []\n",
    "\n",
    "# Read the CSV file\n",
    "with open('disambiguation.csv', mode='r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        label = row[1]\n",
    "        # Ensure the list is large enough to hold the current index\n",
    "        \n",
    "        translate_label.append(label)\n",
    "\n",
    "print(translate_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cpu')\n",
    "PATH = \"Storage\\models\\AutoEncoderModelV2.pth\"\n",
    "if cuda:\n",
    "    model = torch.load(PATH)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "else:    \n",
    "    model = torch.load(PATH, map_location=torch.device('cpu') )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [04:10<00:00,  2.67s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, top_k_accuracy_score\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_out = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs,labels in tqdm(test):\n",
    "        labels = [int(label) for label in labels]\n",
    "        inputs, labels = inputs.to(device), torch.tensor(labels, dtype=torch.long).to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_out.extend(outputs.cpu().numpy())\n",
    "        \n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "all_out = np.array(all_out)\n",
    "cm = confusion_matrix(all_labels, all_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "11994\n",
      "Accuracy: 0.6253126563281641%\n"
     ]
    }
   ],
   "source": [
    "correct = (all_preds==all_labels).sum().item()\n",
    "print(correct)\n",
    "print(len(ImageDataset(dataset=datasets.TEST,network_input_size=256,cuda=cuda)))\n",
    "print(f'Accuracy: {100 * correct / len(ImageDataset(dataset=datasets.TEST,network_input_size=256,cuda=cuda))}%') #TOO BAD!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

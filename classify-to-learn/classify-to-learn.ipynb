{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: progettoVIPM\n"
     ]
    }
   ],
   "source": [
    "if(os.path.split(os.getcwd())[1] == \"classify-to-learn\"):\n",
    "    os.chdir(\"..\")\n",
    "print(\"Current Working Directory: {}\".format(os.path.split(os.getcwd())[1]))\n",
    "\n",
    "cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loadersAndEnums import datasets\n",
    "from utils.loadersAndEnums import networks\n",
    "from utils.loadersAndEnums import ImageDataset\n",
    "from utils.v2 import ExtendedEncoder, Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loadersAndEnums import datasets, ImageDataset\n",
    "from torch.utils.data import DataLoader, ConcatDataset, random_split\n",
    "\n",
    "trainU = ImageDataset(dataset=datasets.TRAINING_UNLABELED,network_input_size=224, cuda=cuda)\n",
    "unlabeled = DataLoader(trainU,64,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=251, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cpu')\n",
    "#PATH = \"Storage\\models\\AutoEncoderModelV2.pth\"\n",
    "PATH = 'Storage\\models\\\\FineTuned_ResNet50_80_Train.pth'\n",
    "model = models.resnet50()\n",
    "model.fc = nn.Linear(model.fc.in_features, 251)\n",
    "if cuda:\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(PATH, map_location=torch.device('cpu') ) )\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_out = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    i=0\n",
    "    for inputs,labels in tqdm(unlabeled):\n",
    "        if i%50==0:\n",
    "            np.array(all_preds).tofile('all_preds_parziale.dat')\n",
    "            np.array(all_out).tofile('all_out_parziale.dat')\n",
    "        labels = [int(label) for label in labels]\n",
    "        inputs, labels = inputs.to(device), torch.tensor(labels, dtype=torch.long).to(device)\n",
    "        outputs = model(inputs)\n",
    "        print('outputs: ', outputs)\n",
    "        print('outputs: ', outputs.shape)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_out.extend(outputs.cpu().numpy())\n",
    "        i+=1\n",
    "        \n",
    "all_preds = np.array(all_preds)\n",
    "all_out = np.array(all_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds.tofile('all_preds_from_resnet_5080.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_out.tofile('all_out_from_resnet_5080.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_outfromfile.shape:  (28477205,)\n",
      "all_outfromfile:  [-3.6530356 -3.1039214 -4.272348  ... -2.616568  -2.5592525 -3.5694938]\n",
      "all_predsfromfile:  [ 94 116 165 ... 195  13 160]\n"
     ]
    }
   ],
   "source": [
    "all_outfromfile = np.fromfile('all_out_from_resnet_5080.dat', dtype=np.float32)\n",
    "print('all_outfromfile.shape: ', all_outfromfile.shape)\n",
    "print('all_outfromfile: ', all_outfromfile)\n",
    "\n",
    "all_predsfromfile = np.fromfile('all_preds_from_resnet_5080.dat',dtype=np.int64)\n",
    "print('all_predsfromfile: ', all_predsfromfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171\n"
     ]
    }
   ],
   "source": [
    "threshold=0.04\n",
    "probs=[]\n",
    "confident_enough=[]\n",
    "confident_enough_classes=[]\n",
    "for i in range(0, len(all_outfromfile), 251):\n",
    "    out_proba_of_one_pic=all_outfromfile[i:i+251]\n",
    "    most_proba = np.argmax(out_proba_of_one_pic)\n",
    "    prob = all_outfromfile[i+most_proba]\n",
    "    prob_norm = prob/np.mean(out_proba_of_one_pic) #to reduce the variance of the prob of the most probable class between images\n",
    "    if prob_norm >= threshold:\n",
    "        confident_enough.append(int(i/251))\n",
    "        confident_enough_classes.append(most_proba)\n",
    "\n",
    "print(len(confident_enough))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total original unlabelled examples: 113455\n",
      "Shape of selected_unlabelled training set:(171, 2)\n",
      "filename of selected_unlabelled saved is: train_cbl.csv\n"
     ]
    }
   ],
   "source": [
    "#Il seguente codice serve per generare un file di annotazione che contenga alcuni unlabeled con la classe predetta dal classificatore\n",
    "train_set_unlabelled = np.loadtxt(datasets.TRAINING_UNLABELED.value[0], delimiter=\",\", dtype=np.str_)\n",
    "print(\"Total original unlabelled examples: {}\".format(train_set_unlabelled.shape[0]))\n",
    "\n",
    "# Select the most confident images predicted's rows\n",
    "selected_unlabelled = train_set_unlabelled[confident_enough, :]\n",
    "selected_unlabelled[:, 1]= confident_enough_classes #change the values in the second column to be the predicted classes\n",
    "print(\"Shape of selected_unlabelled training set:{}\".format(selected_unlabelled.shape))\n",
    "\n",
    "filename = datasets.TRAINING_UNLABELED_MOST_CONFIDENTLY_CLASSIFIED.value[0]\n",
    "np.savetxt(filename, selected_unlabelled,  delimiter = \",\", fmt='%s')\n",
    "print(\"filename of selected_unlabelled saved is: {}\".format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['macaron', 'bignet', 'cruller', 'vongole_clams', 'triangoli_fritti', 'tiramisu', 'mexican_tostadas', 'lamb_moussaka', 'ravioli_cinesi', 'sushi_sashimi', 'knishes', 'crocchette', 'couscous_verdure', 'porridge', 'lattuga_kimchi', 'alga_wakame', 'spaghetti_squash', 'pasta_rigatoni', 'tartare_di_carne', 'cannolo_siciliano', 'fegato_oca', 'cupcakes', 'carne_ossobuco', 'tagliatelle_con_frutti_di_mare', 'canadian_poutine_w_mushroom_gravy', 'ramen', 'panino_pulled_pork', 'ramen_con_uovo_in_mezzo', 'chicken_kiev', 'torta_mele_stile_crostata', 'risotto', 'torta_stile_torrone_canditi', 'maccheroni_con_carne', 'gluten_free_haggis', 'uova_strapazzate_colazione', 'frittata', 'shrimp_scampi', 'sushi_maki', 'orzo_bolognese', 'amish_apple_fritters', 'nachos', 'beef_stroganoff', 'beef_wellington', 'involtino_primavera', 'dolce_rotondo_con_buco', 'gamberoni', 'souffle', 'maiale_cinese', 'muffin', 'uovo_sodo_con_mousse', 'escargots', 'tramezzino_pollo_club_sandwich', 'torta_di_carote', 'falafell', 'farfalle_pasta', 'smoked_duck_terrine', 'uovo_in_camicia', 'gnocchi', 'bubble_n_squek', 'egg_rolls_invol_primav', 'caprese_non_la_pizza', 'sauerkraut', 'creme_brule', 'pavlova', 'fonduta', 'clam', 'jambalaya', 'tempura', 'torta_al_cioccolato', 'chicken_pot_pie', 'pasta_ragu', 'hotpot_giapponese', 'pure_di_mele', 'baklava', 'salisbury_steak', 'spaghetti', 'edamame', 'coq_au_vin', 'tamale', 'mac_n_cheese', 'riso_con_uovo', 'garlic_bread', 'inslata_con_barbabietole', 'tartare_di_carne', 'vermicelli', 'patÃ©_di_pollo', 'pancake', 'chicken_teraggini_stew', 'anelli_di_cipolla', 'red_velvet', 'berry_compote', 'aragosta', 'butter_chicken', 'alette_di_pollo', 'ceaser_salad', 'Succotash', 'hummus', 'fish_n_chips', 'lasagna', 'lutefisk', 'sloppy_joes', 'omino_di_pandizenzero', 'crocchetta_di_pollo_fritto', 'carne_brasato_cube', 'maiale_glassato_cinese', 'guacamole', 'cheddar_sandwich', 'pancake_coi_buchi', 'tacos', 'torta_con_fragole', 'clam_chouder', 'patate_al_forno_con_sotto_il_ragu', 'uovo_sul_pane', 'zuppa_con_formaggio_in_una_ciotolina', 'fettine_di_tartare', 'torta', '???_poi', 'chele_di_granchio', 'uovo_colazione_con_bacon', 'torta_con_glassa', 'custard', 'syllabub', 'porkchop', 'riso_alla_cantonese', 'uovo_sodo', 'chicken_galatine', 'bbq_brisket', 'reuben_sandwich', 'cotoletta', 'ambrosia_salad', 'ravioli_cinesi_abbrustoliti', 'beef_jerky', 'ravioli', 'anelli_di_totano', 'spaghetti_carbonara', 'miso_soup', 'frozen_yoghurt', 'ahi_wonton', 'pannacotta', 'french_toast', 'chicken_enchilada', 'poke', 'fettucine_alfredo', 'chili', 'budino_caramello', 'spiedino_di_pollo', 'sponge_cake', 'casserole', 'riso_frutti_di_mare_cozze', 'blancmange', 'bruschetta', 'tortellini', 'salmone_alla_griglia', 'patatine_fritte', 'shrimp_n_grits', 'churros', 'donout', 'polpettone', 'polpette', 'scrapple', 'strudel', 'torta_bounty', 'torta_swirls', 'filet_mignon', 'hamburger', 'uovo_in_padella', 'tartare', 'pasta_penne', 'egg_benedict', 'torta_di_pane', 'takoyaki', 'smoked_pork_tenderloin', 'mousse_di_cioccolato', 'meringa', 'hotdog', 'cibo_fancy', 'piatti_con_uovo', 'pollo_riso', 'insalata_greca', 'rancheros', 'tagliatelle', 'pomodoro_al_forno_ripieno', 'pasta_al_forno', 'pizza', 'rustici', 'tortilla', 'thai_soup', 'prime_ribs', 'cheesecake', 'clams', 'baked_ziti', 'cozze', 'manicotti', 'gelato', 'waffle', 'ostriche', 'omelette', 'pasta_vongole', 'burrito', 'rotolo', 'zuppa_aragosta', 'grilled_cheese', 'pita_gyro', 'plumcake', 'zuppa_spaghetti_riso', 'panino_aragosta', 'bbq_ribs', 'pate_di_olive', 'chinese_pepper_steak', 'pane_con_formaggio_sopra', 'pilaf', 'foglie_ripiene', 'scaloppine', 'cordon_bleu', 'uovo_denver_omlette', 'alette_di_pollo', 'gamberoni_ripieni', 'carne_con_pepe_cremina', 'frutto_di_mare_non_identificato', 'chifon_cake', 'bignet', 'toad_in_the_hole', 'chicken_marengo', 'victoria_sponge', 'chicken_tamale', 'creampie', 'bastoncini_findus', 'crumb_cake', 'chicken_provencal', 'pizzette', 'bistecca', 'breasato', 'panino', 'scotch_eggs', 'ravioloni', 'peach_melba', 'pear_cake', 'pumpkin_applesauce_cake', 'dont_know', 'torta_a_piu_strati', 'bbq_ribs', 'beef_bourguignon', 'rissoles', 'pasta_al_forno', 'flacky_turnovers', 'canederli', 'cordon_bleu', 'dolcetti_pasta_sfoglia', 'piatto_cinese_verdure_gamberi_agrodolce', 'buffalo_wings', 'pomodoro_al_forno_ripieno']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "translate_label = []\n",
    "\n",
    "# Read the CSV file\n",
    "with open('disambiguation.csv', mode='r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        label = row[1]\n",
    "        # Ensure the list is large enough to hold the current index\n",
    "        \n",
    "        translate_label.append(label)\n",
    "\n",
    "print(translate_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test20DL = ImageDataset(dataset=datasets.VALIDATION_LABELED_20,network_input_size=224, cuda=cuda)\n",
    "test20DL = DataLoader(test20DL,64,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### performance del modello \"base\" (di nostro ha visto l'80% dei dati e basta):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 32.669322709163346%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test20DL:\n",
    "        labels = [int(label) for label in labels]\n",
    "        inputs, labels = inputs.to(device), torch.tensor(labels, dtype=torch.long).to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### uso le immagini dell'unlabeled set del quale resnet è abbastanza confident, per addestrarlo ulteriormente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc = ImageDataset(dataset=datasets.TRAINING_UNLABELED_MOST_CONFIDENTLY_CLASSIFIED,network_input_size=224, cuda=cuda)\n",
    "mcc = DataLoader(mcc,64,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 5.664835646487119\n",
      "Epoch 2/10, Loss: 4.528047784030089\n",
      "Epoch 3/10, Loss: 4.40519306534215\n",
      "Epoch 4/10, Loss: 4.314201609433046\n",
      "Epoch 5/10, Loss: 4.225366486443414\n",
      "Epoch 6/10, Loss: 4.338887576471295\n",
      "Epoch 7/10, Loss: 4.293132589574446\n",
      "Epoch 8/10, Loss: 4.232532307418467\n",
      "Epoch 9/10, Loss: 4.209317850090607\n",
      "Epoch 10/10, Loss: 4.192587769519516\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in mcc:\n",
    "        labels = [int(label) for label in labels]\n",
    "        inputs = inputs.to(device) \n",
    "        labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(mcc)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.398406374501992 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test20DL:\n",
    "        labels = [int(label) for label in labels]\n",
    "        inputs, labels = inputs.to(device), torch.tensor(labels, dtype=torch.long).to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy: {100 * correct / total} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

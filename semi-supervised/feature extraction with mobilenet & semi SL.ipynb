{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Can not perform a '--user' install. User site-packages are not visible in this virtualenv.\n"
     ]
    }
   ],
   "source": [
    "%pip install --user torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: torch in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (2.5.1+cu118)\n",
      "Requirement already satisfied: torchvision in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (0.20.1+cu118)\n",
      "Requirement already satisfied: torchaudio in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (2.5.1+cu118)\n",
      "Requirement already satisfied: filelock in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (from torchvision) (1.26.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: torchsummary in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (1.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (1.26.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (from matplotlib) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: opencv-python in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (from opencv-python) (1.26.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (from scikit-learn) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\uni\\primo semestre 5\\progetto visual\\progettovipm\\env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pickle5\n",
      "  Using cached pickle5-0.0.11.tar.gz (132 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: pickle5\n",
      "  Building wheel for pickle5 (pyproject.toml): started\n",
      "  Building wheel for pickle5 (pyproject.toml): finished with status 'error'\n",
      "Failed to build pickle5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for pickle5 (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [15 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\\lib.win-amd64-cpython-311\\pickle5\n",
      "      copying pickle5\\pickle.py -> build\\lib.win-amd64-cpython-311\\pickle5\n",
      "      copying pickle5\\pickletools.py -> build\\lib.win-amd64-cpython-311\\pickle5\n",
      "      copying pickle5\\__init__.py -> build\\lib.win-amd64-cpython-311\\pickle5\n",
      "      creating build\\lib.win-amd64-cpython-311\\pickle5\\test\n",
      "      copying pickle5\\test\\pickletester.py -> build\\lib.win-amd64-cpython-311\\pickle5\\test\n",
      "      copying pickle5\\test\\test_pickle.py -> build\\lib.win-amd64-cpython-311\\pickle5\\test\n",
      "      copying pickle5\\test\\test_picklebuffer.py -> build\\lib.win-amd64-cpython-311\\pickle5\\test\n",
      "      copying pickle5\\test\\__init__.py -> build\\lib.win-amd64-cpython-311\\pickle5\\test\n",
      "      running build_ext\n",
      "      building 'pickle5._pickle' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for pickle5\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (pickle5)\n"
     ]
    }
   ],
   "source": [
    "##installs pytorch on a cuda-capable windows machine using pip\n",
    "\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "%pip install torchsummary\n",
    "\n",
    "%pip install numpy\n",
    "\n",
    "%pip install matplotlib\n",
    "\n",
    "%pip install opencv-python\n",
    "\n",
    "%pip install scikit-learn\n",
    "\n",
    "%pip install pandas\n",
    "\n",
    "%pip install pickle5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: progettoVIPM\n",
      "Cuda is True\n"
     ]
    }
   ],
   "source": [
    "target_dir = os.path.split(os.getcwd())[1]\n",
    "if(target_dir == \"Transfer_Learning\" or target_dir == \"semi-supervised\"):\n",
    "    os.chdir(\"..\")\n",
    "print(\"Current Working Directory: {}\".format(os.path.split(os.getcwd())[1]))\n",
    "\n",
    "cuda= torch.cuda.is_available()\n",
    "print(\"Cuda is {}\".format(cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loadersAndEnums import datasets\n",
    "from utils.loadersAndEnums import networks\n",
    "from utils.loadersAndEnums import ImageDataset\n",
    "from utils.extractNeuralFeatures import extract_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total labelled examples: 5020\n",
      "Total unlabelled examples: 113455\n",
      "10.0% of unlabelled is: 11345\n",
      "Shape of mixed training set:(16365, 2)\n",
      "Notation file of mixed dataset saved in: train_mixed.csv\n"
     ]
    }
   ],
   "source": [
    "#Il seguente codice serve per generare un file di annotazione fittizzio, che contenga una mistura tra tutti i dati labelled ed alcuni unlabeled\n",
    "train_set_labelled = np.loadtxt(datasets.TRAINING_LABELED.value[0], delimiter=\",\", dtype=np.str_)\n",
    "train_set_unlabelled = np.loadtxt(datasets.TRAINING_UNLABELED.value[0], delimiter=\",\", dtype=np.str_)\n",
    "\n",
    "print(\"Total labelled examples: {}\".format(train_set_labelled.shape[0]))\n",
    "num_rows = train_set_unlabelled.shape[0]\n",
    "print(\"Total unlabelled examples: {}\".format(num_rows))\n",
    "partOfUnlabeledToSelect = 0.10\n",
    "num_samples = int(num_rows * partOfUnlabeledToSelect)\n",
    "print(\"{}% of unlabelled is: {}\".format(partOfUnlabeledToSelect*100, num_samples))\n",
    "np.random.seed(42)\n",
    "# Get random indices\n",
    "random_indices = np.random.choice(num_rows, num_samples, replace=False)\n",
    "\n",
    "# Select the random rows\n",
    "selected_unlabelled = train_set_unlabelled[random_indices, :]\n",
    "\n",
    "train_mixed = np.concatenate((train_set_labelled, selected_unlabelled))#prima era X_train_mobi_n, X_unlabled_mobi_n\n",
    "\n",
    "print(\"Shape of mixed training set:{}\".format(train_mixed.shape))\n",
    "filename = datasets.TRAINING_MIXED.value[0]\n",
    "np.savetxt(filename, train_mixed,  delimiter = \",\", fmt='%s')\n",
    "print(\"Notation file of mixed dataset saved in: {}\".format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found an existing set of features in: ./Storage/neural_features/Train_MobileNetV3_small_minus2_train_set.npy\n",
      "Loading features from file:\n",
      "---------------------------------------------------------------------------------\n",
      "Done feat extraction, total n° of istances in Train: 5020\n",
      "Feature vector shape of Train: (5020, 576)\n",
      "Label vector shape of Train: (5020,)\n",
      "---------------------------------------------------------------------------------\n",
      "Found an existing set of features in: ./Storage/neural_features/Test_MobileNetV3_small_minus2_val_set.npy\n",
      "Loading features from file:\n",
      "---------------------------------------------------------------------------------\n",
      "Done feat extraction, total n° of istances in Test: 11994\n",
      "Feature vector shape of Test: (11994, 576)\n",
      "Label vector shape of Test: (11994,)\n",
      "---------------------------------------------------------------------------------\n",
      "Original classification layers:Sequential(\n",
      "  (0): Linear(in_features=576, out_features=1024, bias=True)\n",
      "  (1): Hardswish()\n",
      "  (2): Dropout(p=0.2, inplace=True)\n",
      "  (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
      ")\n",
      "Classification layers to fine tune:Sequential(\n",
      "  (0): Linear(in_features=576, out_features=1024, bias=True)\n",
      "  (1): Hardswish()\n",
      "  (2): Dropout(p=0.2, inplace=True)\n",
      "  (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# t sta per \"tensor\", ovvero il vettore sulla gpu, mentre \"n\" sta per \"numpy\", ovvero il vettore sulla cpu\n",
    "chosen_net = networks.MOBILENET\n",
    "layers_to_remove = 2\n",
    "X_train_mobi_t, X_train_mobi_n, y_train, X_test_mobi_t, X_test_mobi_n, y_test, fine_tune_layers = extract_features(\n",
    "                                                                                            cuda=cuda,\n",
    "                                                                                            transform=None,\n",
    "                                                                                            train_set=datasets.TRAINING_LABELED,\n",
    "                                                                                            test_set=datasets.TEST,\n",
    "                                                                                            network=chosen_net,\n",
    "                                                                                            layers_to_remove=layers_to_remove)\n",
    "print(\"Original classification layers:{}\".format(chosen_net.value[1].classifier[:]))\n",
    "print(\"Classification layers to fine tune:{}\".format(fine_tune_layers[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not find an existing set of features in: ./Storage/neural_features/Train_MobileNetV3_small_minus2_train_mixed.npy\n",
      "Extracting features:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16365/16365 [03:16<00:00, 83.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying features to cpu:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16365/16365 [00:00<00:00, 76076.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Done feat extraction, total n° of istances in Train: 16365\n",
      "Feature vector shape of Train: (16365, 576)\n",
      "Label vector shape of Train: (16365,)\n",
      "---------------------------------------------------------------------------------\n",
      "Found an existing set of features in: ./Storage/neural_features/Test_MobileNetV3_small_minus2_val_set.npy\n",
      "Loading features from file:\n",
      "---------------------------------------------------------------------------------\n",
      "Done feat extraction, total n° of istances in Test: 11994\n",
      "Feature vector shape of Test: (11994, 576)\n",
      "Label vector shape of Test: (11994,)\n",
      "---------------------------------------------------------------------------------\n",
      "Original classification layers:Sequential(\n",
      "  (0): Linear(in_features=576, out_features=1024, bias=True)\n",
      "  (1): Hardswish()\n",
      "  (2): Dropout(p=0.2, inplace=True)\n",
      "  (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
      ")\n",
      "Classification layers to fine tune:Sequential(\n",
      "  (0): Linear(in_features=576, out_features=1024, bias=True)\n",
      "  (1): Hardswish()\n",
      "  (2): Dropout(p=0.2, inplace=True)\n",
      "  (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# t sta per \"tensor\", ovvero il vettore sulla gpu, mentre \"n\" sta per \"numpy\", ovvero il vettore sulla cpu\n",
    "chosen_net = networks.MOBILENET\n",
    "layers_to_remove = 2\n",
    "X_mixed_mobi_t, X_mixed_mobi_n, mixed_labels, X_test_mobi_t, X_test_mobi_n, y_test, fine_tune_layers = extract_features(cuda=cuda,\n",
    "                                                                                        transform=None,\n",
    "                                                                                        train_set=datasets.TRAINING_MIXED,\n",
    "                                                                                        test_set=datasets.TEST,\n",
    "                                                                                        network=chosen_net,\n",
    "                                                                                        layers_to_remove=layers_to_remove)\n",
    "print(\"Original classification layers:{}\".format(chosen_net.value[1].classifier[:]))\n",
    "print(\"Classification layers to fine tune:{}\".format(fine_tune_layers[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def model_building(model, model_name, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc=accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=model.classes_)\n",
    "    disp.plot()\n",
    "    plt.savefig(\"./semi-supervised/model_metrics/ConfM_{}.pdf\".format(model_name))\n",
    "    cm = np.array(cm)\n",
    "    np.save(\"./semi-supervised/model_metrics/ConfM_{}.npy\".format(model_name), cm)\n",
    "    with open('./semi-supervised/models/{}.pkl'.format(model_name),'wb') as f:\n",
    "        pickle.dump(model,f)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finito di inferire le labels sugli non annotati\n",
      "finito di fittare il classificatore\n",
      "Accuracy: 6.053\n"
     ]
    }
   ],
   "source": [
    "from sklearn.semi_supervised import LabelSpreading\n",
    "\n",
    "model  = [LabelSpreading(kernel=\"knn\", max_iter=5, n_jobs=-1, n_neighbors=3), \"LabelSpreading\"]\n",
    "model[0].fit(X_mixed_mobi_n, mixed_labels)\n",
    "print(\"finito di inferire le labels sugli non annotati\")\n",
    "\n",
    "guessedAndRealLabels = model[0].transduction_\n",
    "\n",
    "\n",
    "model2 = RandomForestClassifier()\n",
    "model2.fit(X_mixed_mobi_n, guessedAndRealLabels) \n",
    "print(\"finito di fittare il classificatore\")\n",
    "guessedTestLabels = model2.predict(X_test_mobi_n)\n",
    "score = accuracy_score(y_test, guessedTestLabels)\n",
    "cm = confusion_matrix(y_test, guessedTestLabels)\n",
    "# summarize score\n",
    "print('Accuracy: %.3f' % (score*100))\n",
    "np.save(\"./semi-supervised/model_metrics/Accuracies_{}_minus{}_randomForest_labelSpreading_{}_ofunlabeled.npy\".format(chosen_net, layers_to_remove, partOfUnlabeledToSelect), score)\n",
    "np.save(\"./semi-supervised/model_metrics/ConfM_{}_minus{}_randomForest_labelSpreading_{}_ofunlabeled.npy\".format(chosen_net, layers_to_remove, partOfUnlabeledToSelect), score)\n",
    "# 5% unlabeled ->6.9 di acc\n",
    "# 10% unlabeled ->6.9 di acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
